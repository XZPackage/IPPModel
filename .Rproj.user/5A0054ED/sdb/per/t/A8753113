{
    "contents" : "#' Class providing object with methods for drawing IPPs and FIN\n#'\n#' The class provides objects with methods for drawing impact pattern plots (IPPs) and feature interaction network (FIN).\n#' @aliases IPP FIN\n#' @docType class\n#'\n#' @export\n#' @keywords data\n#'\n#' @return Object of \\code{\\link{R6Class}} with methods for drawing IPPs and FIN.\n#'\n#' @format \\code{\\link{R6Class}} object.\n#'\n#' @field \\link{X.Data} a data.frame, the dataset of input features.\n#' @field \\link{Pred.Fun} an object, the prediction function. It can be any\n#'   model created by \"nnet\", \"randomforest\" and \"kernlab\" etc.\n#' @field \\link{Model.Package} a string, the package name of the interpreted\n#'   machine learning model, such as \"nnet\" and \"randomforest\".\n#' @field \\link{Pred.Type} a string, the type of prediction.\n#' @field \\link{Pred.Dimension} an integer, indicating which class is predicted.\n#'   This field is used only for classification model.\n#' @field \\link{XB.Size} an integer, the size of \\link{XB.Sample}.\n#' @field \\link{XB.SamplingMethod} a string, the sampling method of\n#'   \\link{XB.Sample}, \"joint\" or \"independent\". \"joint\" means that all features\n#'   are sampled from \\link{X.Data} jointly. \"independent\" means that each\n#'   feature is sampled independently; then all features are combined randomly.\n#' @field \\link{ParaTable} a data.frame, the parameter table. It is generated by\n#'   method \\link{GenerateParaTable}.\n#' @field \\link{XA.Sample} a list, the sample of X_A extracted from\n#'   \\link{X.Data}. It is generated by method \\link{SamplingXA}.\n#' @field \\link{XB.Sample} a list, the sample of X_B extracted from\n#'   \\link{X.Data}. It is generated by method \\link{SamplingXB}.\n#' @field \\link{Pred.Res} a list, the prediction results of f(X_A,X_B), which is\n#'   generated by method \\link{PredictData}.\n#' @field \\link{Clustering.Res} a list, the clustering results, which is\n#'   generated by method \\link{ClusterImpactPlots}.\n#' @field \\link{TreeRules} a list, the decision tree rules, which is generated\n#'   by method \\link{BuildTree}.\n#' @field \\link{FIN.Data} a data.frame, the feature interaction network, which\n#'   is generated by method \\link{BuildTree}.\n#' @field \\link{ColorList} a list, the curve colors used for drawing IPPs.\n#' @field \\link{TaskFinishTime} a list, the finishing time of tasks.\n#' @section Methods: \\describe{ \\item{\\link{initialize}}{initialize some fields\n#'   of object and excute the method \\link{CheckInitialization} and\n#'   \\link{GenerateParaTable}.} \\item{\\link{CheckInitialization}}{validate the\n#'   initialization information.} \\item{\\link{GenerateParaTable}}{Generate the\n#'   parameter table \\link{ParaTable}. } \\item{\\link{CheckParaTable}}{validate\n#'   the information in \\link{ParaTable}.} \\item{\\link{SamplingXA}}{sampling\n#'   \\link{XA.Sample} from \\link{X.Data}.} \\item{\\link{SamplingXB}}{sampling\n#'   \\link{XB.Sample} from \\link{X.Data}.} \\item{\\link{PredictData}}{predict\n#'   data using \\link{Pred.Fun} based on \\link{XA.Sample} and \\link{XB.Sample}.}\n#'   \\item{\\link{ClusterImpactPlots}}{cluster the impact curves of each feature\n#'   based on the predicting results \\link{Pred.Res}.}\n#'   \\item{\\link{BuildTree}}{build decision tree based on the clustering results\n#'   \\link{Clustering.Res}.} \\item{\\link{DrawIPP}}{draw the impact pattern\n#'   plots.} \\item{\\link{DrawFIN}}{draw the feature interaction network.}\n#'   \\item{\\link{WriteToExcel}}{write the results to an excel file.}\n#'   \\item{\\link{ExecuteAll}}{execute the methods \\link{SamplingXA},\n#'   \\link{SamplingXB}, \\link{PredictData}, \\link{ClusterImpactPlots} and\n#'   \\link{BuildTree} in sequence.}\n#'\n#'   }\n#' @examples\n#' library(IPPModel)\n#' library(igraph)\n#'\n#' #------ FIRST EXAMPLE ------\n#' library(nnet)\n#' data(\"bank\")\n#' # build model\n#' bank.NN <- nnet(y ~ ., data = bank, size = 5, maxit = 1000)\n#' # remove the output variable\n#' bank.ds = bank[-17]\n#' # create IPPModel object\n#' IPP.bank = IPPModel$new(XDS=bank.ds, PredFun=bank.NN,\n#'                         ModelPackage=\"nnet\", PredType=\"raw\", PredDim=1,\n#'                         XB.Size=1000, XB.SamplingMethod=\"joint\")\n#' # modify the clustering method to \"kmedoids\"\n#' IPP.bank$ParaTable$clusteringMethod = \"kmedoids\"\n#' # execute all tasks\n#' IPP.bank$ExecuteAll()\n#' # draw impact pattern plots (IPP)\n#' IPP.bank$DrawIPP(centralized = TRUE, nc = 4)\n#' # draw feature interaction network (FIN)\n#' IPP.bank$DrawFIN(threshold = 0.2, lay.out = igraph::layout.auto)\n#' # write the results into an excel file\n#' IPP.bank$WriteToExcel(\"output.xlsx\")\n#'\n#' #------ SECOND EXAMPLE ------\n#' library(randomForest)\n#' data(\"whitewine\")\n#' # build model\n#' WW.RF <- randomForest(quality ~ ., data = whitewine, mtry = 4,importance=TRUE, na.action=na.omit)\n#' # remove the output variable\n#' WW.ds = whitewine[-12]\n#' # create IPPModel object\n#' IPP.WW = IPPModel$new(XDS=WW.ds, PredFun=WW.RF,\n#'                       ModelPackage=\"randomForest\", PredType=\"response\", PredDim=1,\n#'                       XB.Size=1000, XB.SamplingMethod=\"joint\")\n#' # set the maximum depth of trees to be 5\n#' IPP.WW$ParaTable$treeDepth = 5\n#' # execute all tasks\n#' IPP.WW$ExecuteAll()\n#' # draw impact pattern plots (IPP)\n#' IPP.WW$DrawIPP(centralized = TRUE, nc = 4)\n#' # draw feature interaction network (FIN)\n#' IPP.WW$DrawFIN(threshold = 0.1, lay.out = igraph::layout.circle)\n#'\n#' #------ THIRD EXAMPLE ------\n#' library(kernlab)\n#' data(\"iris\")\n#' iris.SVM <- ksvm(Species ~ ., data = iris,kernel=\"rbfdot\", kpar=\"automatic\",C=0.1, prob.model = TRUE)\n#' # remove the output variable\n#' iris.ds = iris[-5]\n#' # create IPPModel object\n#' IPP.iris = IPPModel$new(XDS=iris.ds, PredFun=iris.SVM,\n#'                         ModelPackage=\"kernlab\", PredType=\"prob\", PredDim=1,\n#'                         XB.Size=200, XB.SamplingMethod=\"independent\")\n#' # execute the tasks step by step\n#' IPP.iris$SamplingXA()  # sampling XA\n#' IPP.iris$SamplingXB()  # sampling XB\n#' IPP.iris$PredictData()  # predict\n#' IPP.iris$ClusterImpactPlots() # clustering impact plots\n#' IPP.iris$BuildTree()  # build tree\n#' # draw impact pattern plots (IPP)\n#' IPP.iris$DrawIPP(centralized = TRUE, nc = 4)\n#' # draw feature interaction network (FIN)\n#' IPP.iris$DrawFIN(threshold = 0.3, lay.out = igraph::layout.auto)\n#' # write the results into an excel file\n#' IPP.iris$WriteToExcel(\"output.xlsx\")\n#'\n#' @references Xiaohang Zhang, Ji Zhu, SuBang Choe, Yi Lu and Jing Liu.\n#'   Exploring black box of supervised learning models: Visualizing the impact\n#'   of features on prediction. Working paper.\n#'\n\nIPPModel <- R6::R6Class(\n    \"IPPModel\",\n    public = list(\n        ################# ATTRIBUTES ################\n\n        X.Data = NA, # the dataset of input features\n        Pred.Fun = NA, # the prediction function\n        Pred.Type = NA, # the type of prediction\n        Pred.Dimension = NA, # which class is predicted\n        Model.Package = NA, # the package name of the interpreted machine learnign model\n        XB.Size = NA, # the size of XB.Sample\n        XB.SamplingMethod = NA, # sampling method of XB.Sample:\n            #  - \"joint\": all features are sampled jointly from X.Data\n            #  - \"indepedent\": all features are sampled independely from X.Data\n\n        ParaTable = NA, # the information of X.Data, which is generated by GenerateParaInfo()\n        XA.Sample = NA, # the sample of X_A extracted from X.Data, which is generate by SamplingXA()\n        XB.Sample = NA, # the sample of X_B extracted from X.Data, which is generate by SamplingXB()\n\n        Pred.Res = NA, # prediction results of f(X_A,X_B), which is generate by DataPrediction()\n        Clustering.Res = NA, # clustering results, which is generate by Clustering()\n        TreeRules = NA, # decision tree rules, which is generated by BuildingTree()\n        FIN.Data = NA, # feature interaction networks, which is generated by BuildingTree()\n\n        TaskFinishTime = NA, # the finish time of tasks\n        ColorList = c(\"red\", \"darkgreen\", \"blue\", \"orange\", \"black\", \"purple\", \"pink\",\n                      \"cyan\", \"cadeblue\"), # the line colors used for drawing IPPs\n\n        ################ METHODS ################\n\n        #------------------------------------\n        # initialization\n        #------------------------------------\n\n        initialize = function(XDS, PredFun, PredType, PredDim=1, ModelPackage, XB.Size=200,\n                              XB.SamplingMethod=\"joint\"){\n            cat(\"...Initializing\\n\")\n\n            if(!self$CheckInitialization(XDS, PredType, PredDim, ModelPackage, XB.Size, XB.SamplingMethod)){\n                stop(\"Parameter initialization error\")\n            }\n\n            require(ModelPackage, character.only = TRUE)\n            x = predict(PredFun, XDS[1,], type = PredType)\n            if(!is.numeric(x)){\n                stop(\"PredFun, XDS and PredType do not match.\")\n            }\n\n            self$X.Data = XDS\n            self$XB.Size = XB.Size\n            self$XB.SamplingMethod = XB.SamplingMethod\n            self$Pred.Fun = PredFun\n            self$Pred.Type = PredType\n            self$Pred.Dimension = PredDim\n            self$Model.Package = ModelPackage\n\n\n\n            self$GenerateParaTable()\n            self$TaskFinishTime[['Initializ']] = Sys.time()\n            cat(\"   ...Finished\\n\")\n            cat(\"...Please check the parameters by accessing the attribute 'ParaTable' before continuing the other tasks.\\n\")\n        },\n\n        #------------------------------------\n        # check initialization information\n        #------------------------------------\n        CheckInitialization = function(XDS, PredType, PredDim, ModelPackage, XB.Size, XB.SamplingMethod){\n            if(!is.data.frame(XDS)){\n                message(\"...XDS must be a data.frame.\")\n                return (FALSE)\n            }\n\n            check.integer = function(x) x == round(x)\n\n            if(!check.integer(PredDim)){\n                message(\"...PreDim must be an integer.\")\n                return (FALSE)\n            }else if(PredDim < 1){\n                message(\"...PredDim must be an integer greater than 0.\")\n                return (FALSE)\n            }\n\n            if(!check.integer(XB.Size)){\n                message(\"...XB.Size must be an integer.\")\n                return (FALSE)\n            }else if(XB.Size < 1){\n                message(\"...XB.Size must be an integer greater than 0.\")\n                return (FALSE)\n            }\n\n            if(! XB.SamplingMethod %in% c(\"joint\",\"independent\")){\n                message(\"...XB.SamplingMethod must be 'joint' or 'independent'.\")\n                return (FALSE)\n            }\n\n            if(!is.character(PredType)){\n                message(\"...PredType must a string.\")\n                return (FALSE)\n            }\n\n            if(!is.character(ModelPackage)){\n                message(\"...ModelPackage must a string.\")\n                return (FALSE)\n            }\n\n            return (TRUE)\n        },\n\n        #------------------------------------\n        # generate parameter information\n        #------------------------------------\n        GenerateParaTable = function(){\n            varNum = ncol(self$X.Data)\n\n            varNames = names(self$X.Data)\n            uniqValue = rep(NA,varNum) # the number of unique values\n            dataType = rep(NA,varNum) # the data type\n\n            X_A = rep(TRUE,varNum); # a target feature or not\n            samplingMethod = rep(\"all\",varNum); # sampling method\n            L_A = rep(NA,varNum) # the number of levels\n\n            centralized = rep(TRUE,varNum); # centralized or not\n            distMeasure = rep(NA,varNum) # the distance measure\n            autoK = rep(TRUE,varNum); # if the number of cluster is determined automatically\n            numK = rep(5,varNum); # the number of clusters if autoK is FALSE or the max number of cluster if autoK is TRUE\n            clusteringMethod = rep(\"kmeans\", varNum) # clustering method\n\n            treeDepth = rep(3,varNum) # the maximum depth of decision tree\n            minSplit = rep(60,varNum) # the minimum number of observations for splitting\n\n            for(i in 1:varNum){\n                uniqValue[i] = length(unique(self$X.Data[,i]))\n                if(is.factor(self$X.Data[,i])){\n                    if(is.ordered(self$X.Data[,i])){\n                        dataType[i] = \"ordinal\"\n                        L_A[i] = 0\n                        distMeasure[i] = \"euclidean\"\n                    }else if(uniqValue[i] == 2){\n                        dataType[i] = \"binary\"\n                        L_A[i] = 0\n                        distMeasure[i] = \"euclidean\"\n                    }else{\n                        dataType[i] = \"nominal\"\n                        L_A[i] = 0\n                        distMeasure[i] = \"euclidean\"\n                    }\n                }else if(uniqValue[i] > 10){\n                    dataType[i] = \"interval\"\n                    L_A[i] = 50\n                    samplingMethod[i] = \"equal\"\n                    distMeasure[i] = \"cosine\"\n                }else{\n                    dataType[i] = \"ordinal\"\n                    L_A[i] = 0\n                    distMeasure[i] = \"euclidean\"\n                }\n            }\n            XInfo = data.frame(dataType,uniqValue,\n                               X_A,L_A,samplingMethod,\n                               clusteringMethod, centralized,distMeasure,autoK,numK,\n                               treeDepth,minSplit)\n            rownames(XInfo) = varNames\n            # set levels\n            levels(XInfo$dataType) = c(levels(XInfo$dataType), setdiff(c('interval', 'binary', 'nominal', 'ordinal'), levels(XInfo$dataType)))\n            levels(XInfo$X_A) = c(levels(XInfo$X_A), setdiff(c(TRUE, FALSE), levels(XInfo$X_A)))\n            levels(XInfo$samplingMethod) = c(levels(XInfo$samplingMethod), setdiff(c('all', 'equal', 'percentile', 'random'), levels(XInfo$samplingMethod)))\n            levels(XInfo$distMeasure) = c(levels(XInfo$distMeasure), setdiff(c('cosine','euclidean'), levels(XInfo$distMeasure)))\n            levels(XInfo$centralized) = c(levels(XInfo$centralized), setdiff(c(TRUE, FALSE), levels(XInfo$centralized)))\n            levels(XInfo$autoK) = c(levels(XInfo$autoK), setdiff(c(TRUE, FALSE), levels(XInfo$autoK)))\n            levels(XInfo$clusteringMethod) = c(levels(XInfo$clusteringMethod), setdiff(c(\"kmeans\", \"kmedoids\"), levels(XInfo$clusteringMethod)))\n\n            self$ParaTable = XInfo\n            self$TaskFinishTime[['ParaTable']] = Sys.time()\n        },\n\n        #------------------------------------\n        # check parameter information\n        #------------------------------------\n        CheckParaTable = function(){\n            if(!all(row.names(self$ParaTable) %in% names(self$X.Data))){\n                message(\"The row names of ParaTable are not consistent with the column names of X.Data.\")\n                return (FALSE)\n            }\n            if(!all(unique(self$ParaTable[,\"dataType\"]) %in%\n                    c('interval', 'binary', 'nominal', 'ordinal'))){\n                message(\"The 'dataType' in ParaTable must be 'interval', 'binary', 'nominal' or 'ordinal'.\")\n                return (FALSE)\n            }\n\n            if(!all(unique(self$ParaTable[,\"X_A\"]) %in%\n                    c(TRUE, FALSE))){\n                message(\"The 'X_A' in ParaTable must be TRUE or FALSE.\")\n                return (FALSE)\n            }\n\n            if(!all(unique(self$ParaTable[,\"centralized\"]) %in% c(TRUE, FALSE))){\n                message(\"The 'centralized' in ParaTable must be TRUE or FALSE.\")\n                return (FALSE)\n            }\n\n            if(!all(unique(self$ParaTable[,\"autoK\"]) %in% c(TRUE, FALSE))){\n                message(\"The 'autoK' in ParaTable must be TRUE or FALSE.\")\n                return (FALSE)\n            }\n\n            if(!all(unique(self$ParaTable[,\"samplingMethod\"]) %in% c('all', 'equal', 'percentile', 'random'))){\n                message(\"The 'samplingMethod' in ParaTable must be 'all', 'equal', 'percentile' or 'random'.\")\n                return (FALSE)\n            }\n\n            if(!all(unique(self$ParaTable[,\"clusteringMethod\"]) %in% c(\"kmeans\", \"kmedoids\"))){\n                message(\"The 'clusteringMethod' in ParaTable must be 'kmeans' or 'kmedoids'.\")\n                return (FALSE)\n            }\n\n            if(!all(unique(self$ParaTable[,\"distMeasure\"]) %in% c(\"cosine\", \"euclidean\"))){\n                message(\"The 'distMeasure' in ParaTable must be 'cosine' or 'euclidean'.\")\n                return (FALSE)\n            }\n\n            check.integer = function(x) x == round(x)\n\n            if(!all(check.integer(self$ParaTable[,\"L_A\"]))){\n                message(\"The 'L_A' in ParaTable must be integers.\")\n                return (FALSE)\n            }else if(!all(self$ParaTable[,\"L_A\"] >= 0)){\n                message(\"The 'L_A' in ParaTable must be non-negative integers.\")\n                return (FALSE)\n            }\n\n            if(!all(check.integer(self$ParaTable[,\"numK\"]))){\n                message(\"The 'numK' in ParaTable must be integers.\")\n                return (FALSE)\n            }else if(!all(self$ParaTable[,\"numK\"] >= 1)){\n                message(\"The 'numK' in ParaTable must be integers greater than 0.\")\n                return (FALSE)\n            }\n\n            if(!all(check.integer(self$ParaTable[,\"treeDepth\"]))){\n                message(\"The 'treeDepth' in ParaTable must be integers.\")\n                return (FALSE)\n            }else if(!all(self$ParaTable[,\"treeDepth\"] >= 1)){\n                message(\"The 'treeDepth' in ParaTable must be integers greater than 0.\")\n                return (FALSE)\n            }\n\n            if(!all(check.integer(self$ParaTable[,\"minSplit\"]))){\n                message(\"The 'minSplit' in ParaTable must be integers.\")\n                return (FALSE)\n            }else if(!all(self$ParaTable[,\"minSplit\"] >= 1)){\n                message(\"The 'minSplit' in ParaTable must be integers greater than 0.\")\n                return (FALSE)\n            }\n\n            return (TRUE)\n        },\n\n        #------------------------------------\n        # sampling X_A\n        #------------------------------------\n        SamplingXA = function(){\n            cat(\"...Sampling X_A\\n\")\n            if(!self$CheckInitialization(self$X.Data, self$Pred.Type, self$Pred.Dimension, self$Model.Package, self$XB.Size, self$XB.SamplingMethod)){\n                stop(\"Parameter initialization error\")\n            }\n            if(!self$CheckParaTable()){\n                stop(\"The parameters in ParaTable are wrong.\")\n            }\n            X_A = list()\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n            xnames = names(self$X.Data)\n            for(i in 1:length(AList)){\n                var = AList[i]\n                LA = self$ParaTable[var, \"L_A\"]\n                if(self$ParaTable[var, \"dataType\"] == \"interval\"){\n                    sM = self$ParaTable[var, \"samplingMethod\"]\n                    # X_A is sampled uniformly\n                    if(LA == 0){\n                        X_A[[var]] = sort(unique(self$X.Data[,var]))\n                    }else if(sM == 'equal'){\n                        minV = min(self$X.Data[,var],na.rm = TRUE)\n                        maxV = max(self$X.Data[,var],na.rm = TRUE)\n                        X_A[[var]] = seq(minV,maxV,(maxV - minV) / (LA - 1))\n                    }else if(sM == 'percentile'){\n                        X_A[[var]] = quantile(self$X.Data[,var], probs = seq(0, 1, 1 / (LA - 1)))\n                    }else{ # random\n                        X_A[[var]] = sort(sample(unique(self$X.Data[,var]), LA))\n                    }\n                }else{\n                    # X_A is sampled from dataset\n                    temp = unique(self$X.Data[,var])\n                    if(LA == 0 | length(temp) <= LA)\n                        X_A[[var]] = sort(temp)\n                    else\n                        X_A[[var]] = sort(sample(temp, LA))\n                }\n            }\n            self$XA.Sample = X_A\n            self$TaskFinishTime[['XA.Sample']] = Sys.time()\n            cat(\"   ...Finished\\n\")\n        },\n\n        #------------------------------------\n        # sampling X_B\n        #------------------------------------\n        SamplingXB = function(){\n            cat(\"...Sampling X_B\\n\")\n\n            if(!self$CheckInitialization(self$X.Data, self$Pred.Type, self$Pred.Dimension, self$Model.Package, self$XB.Size, self$XB.SamplingMethod)){\n                stop(\"Parameter initialization error\")\n            }\n            if(!self$CheckParaTable()){\n                stop(\"The parameters in ParaTable are wrong.\")\n            }\n\n            if(self$XB.SamplingMethod == \"joint\"){\n                # All features in X_B are sampled directly as a whole from dataset\n                # while the joint distribution X_B keeps unchanged.\n                if(self$XB.Size >= nrow(self$X.Data)){\n                    self$XB.Sample = self$X.Data\n                }else{\n                    self$XB.Sample = self$X.Data[sample(1:nrow(self$X.Data), self$XB.Size),]\n                }\n            }else{\n                # Each feature in X_B is sampled independently.\n                X_B = data.frame(matrix(NA, nrow = self$XB.Size, ncol = ncol(self$X.Data)))\n                names(X_B) = names(self$X.Data)\n                for(fea in names(self$X.Data)){\n                    X_B[[fea]] = sample(self$X.Data[[fea]], self$XB.Size, replace = TRUE)\n                }\n                self$XB.Sample = X_B\n            }\n            self$TaskFinishTime[['XB.Sample']] = Sys.time()\n            cat(\"   ...Finished\\n\")\n        },\n\n        #------------------------------------\n        # prediction of X_A and X_B\n        #------------------------------------\n        PredictData = function(){\n            cat(\"...Predicting\\n\")\n\n            if(!self$CheckInitialization(self$X.Data, self$Pred.Type, self$Pred.Dimension, self$Model.Package, self$XB.Size, self$XB.SamplingMethod)){\n                stop(\"Parameter initialization error\")\n            }\n            if(!self$CheckParaTable()){\n                stop(\"The parameters in ParaTable are wrong.\")\n            }\n            if(!is.list(self$XA.Sample)){\n                stop(\"XA.Sample data is not correct.\")\n            }else if(!all(names(self$XA.Sample) %in% row.names(self$ParaTable))){\n                stop(\"XA.Sample data is not correct.\")\n            }\n            if(!is.data.frame(self$XB.Sample)){\n                stop(\"XB.Sample data is not correct.\")\n            }else if(!all(names(self$XB.Sample) %in% row.names(self$ParaTable))){\n                stop(\"XB.Sample data is not correct.\")\n            }\n\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n\n            cluster <- parallel::makeCluster(length(AList))\n            parallel::clusterExport(cl=cluster, varlist=c(\"predict\"), envir = environment())\n            res = parallel::parLapply(cl = cluster,X = 1:length(AList), fun = private$DataPredictionSingle)\n            parallel::stopCluster(cluster)\n            names(res) = AList\n            self$Pred.Res = res\n            self$TaskFinishTime[['Pred.Res']] = Sys.time()\n            cat(\"   ...Finished\\n\")\n        },\n\n        #------------------------------------\n        # clustering based on K-medoid\n        #------------------------------------\n        ClusterImpactPlots = function(){\n            cat(\"...Clustering\\n\")\n\n            if(!self$CheckInitialization(self$X.Data, self$Pred.Type, self$Pred.Dimension, self$Model.Package, self$XB.Size, self$XB.SamplingMethod)){\n                stop(\"Parameter initialization error\")\n            }\n            if(!self$CheckParaTable()){\n                stop(\"The parameters in ParaTable are wrong.\")\n            }\n\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n\n            if(!is.list(self$Pred.Res)){\n                stop(\"Pred.Res data is not correct. You need to predict first before clustering.\")\n            }else if(!all(names(self$Pred.Res) %in% AList)){\n                stop(\"Pred.Res data is not correct. You need to predict first before clsutering.\")\n            }\n\n            # parallel clustering\n            cluster <- parallel::makeCluster(length(AList))\n            parallel::clusterExport(cl=cluster, varlist=c(), envir = environment())\n            res = parallel::parLapply(cl = cluster,X = 1:length(AList), fun = private$ClusteringSingle)\n            parallel::stopCluster(cluster)\n\n            names(res) = AList\n            self$Clustering.Res = res\n            self$TaskFinishTime[['Clustering.Res']] = Sys.time()\n            cat(\"   ...Finished\\n\")\n        },\n\n        #------------------------------------\n        # build decision tree\n        #------------------------------------\n        BuildTree = function(){\n            cat(\"...Building Decision Trees\\n\")\n\n            if(!self$CheckInitialization(self$X.Data, self$Pred.Type, self$Pred.Dimension, self$Model.Package, self$XB.Size, self$XB.SamplingMethod)){\n                stop(\"Parameter initialization error\")\n            }\n            if(!self$CheckParaTable()){\n                stop(\"The parameters in ParaTable are wrong.\")\n            }\n\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n\n            if(!is.list(self$Clustering.Res)){\n                stop(\"Clustering.Res data is not correct. You need to cluster first before building trees.\")\n            }else if(!all(names(self$Clustering.Res) %in% AList)){\n                stop(\"Clustering.Res data is not correct. You need to cluster first before building trees.\")\n            }\n\n\n            cluster <- parallel::makeCluster(length(AList))\n            parallel::clusterExport(cl=cluster, varlist=c(), envir = environment())\n            res = parallel::parLapply(cl = cluster,X = 1:length(AList), fun = private$BuildingTreeSingle)\n            parallel::stopCluster(cluster)\n            names(res) = AList\n            self$TreeRules = res\n\n            # Build feature interaction network (FIN)\n            self$FIN.Data = data.frame(matrix(NA, nrow = ncol(self$X.Data), ncol = length(AList)),\n                                  row.names = names(self$X.Data))\n            colnames(self$FIN.Data) = AList\n            for(fea in AList){\n                varImp = res[[fea]][['varImp']]\n                self$FIN.Data[names(varImp), fea] = varImp\n            }\n\n            self$TaskFinishTime[['TreeRules']] = Sys.time()\n            self$TaskFinishTime[['FIN.Data']] = Sys.time()\n            cat(\"   ...Finished\\n\")\n        },\n\n        #------------------------------------\n        # draw impact pattern plots (IPP)\n        #------------------------------------\n        DrawIPP = function(centralized=TRUE, nc=4){\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n\n            if(!is.list(self$Clustering.Res)){\n                stop(\"Clustering.Res data is not correct. You need to cluster first before drawing IPPs.\")\n            }else if(!all(names(self$Clustering.Res) %in% AList)){\n                stop(\"Clustering.Res data is not correct. You need to cluster first before drawing IPPs.\")\n            }\n\n            nr = ceiling(length(AList) / nc)\n            par(mfcol = c(nr, nc), mai = c(.7,.5,.3,.5))\n            for(fea in AList){\n                IPPs = self$Clustering.Res[[fea]][[\"medoids\"]]\n                if(centralized){\n                    IPPs = IPPs - rowMeans(IPPs)\n                }\n                bestK = self$Clustering.Res[[fea]][[\"bestK\"]]\n                XA = self$XA.Sample[[fea]]\n\n                ymax = max(IPPs)\n                ymin = min(IPPs)\n\n                if(bestK > 1){\n                    y = IPPs[1,]\n                }else{\n                    y = IPPs[,1]\n                }\n\n                if(self$ParaTable[fea,\"dataType\"] == \"interval\"){\n\n                    plot(x = XA, y = y, type = \"l\", xlab = fea,\n                         ylab = \"prediction\", ylim = c(ymin, ymax), col = self$ColorList[1])\n                }else{\n                    plot(x = XA, y = y, type = \"l\", xlab = fea, xaxt = \"n\",\n                         ylab = \"prediction\", ylim = c(ymin, ymax), col = self$ColorList[1])\n                    axis(1, labels = as.character(XA), at = as.numeric(XA))\n                }\n\n                if(bestK > 1){\n                    for(i in 2:bestK){\n                        if(i > length(self$ColorList))\n                            coll = \"black\"\n                        else\n                            coll = self$ColorList[i]\n                        lines(x = XA, y = IPPs[i,], type = \"l\", col = coll)\n                    }\n                }\n            }\n            #dev.off()\n            par(mfcol = c(1, 1))\n        },\n\n        #------------------------------------\n        # write results to excel file\n        #------------------------------------\n        WriteToExcel = function(excelName){\n            cat(\"...Write Information\\n\")\n            require(\"xlsx\",character.only = TRUE)\n            wb <- xlsx::createWorkbook()\n\n            cs1 <- xlsx::CellStyle(wb) + xlsx::Font(wb, isBold = TRUE, color = \"red\")\n            cs2 <- xlsx::CellStyle(wb) + xlsx::Font(wb, isItalic = TRUE, color = \"blue\")\n\n            # write the parameter information\n            sheet  <- xlsx::createSheet(wb, sheetName=\"Parameters\")\n            xlsx::addDataFrame(x = self$ParaTable, sheet = sheet,\n                               row.names = TRUE, col.names = TRUE,\n                               startColumn = 1, startRow = 1,\n                               rownamesStyle = cs2, colnamesStyle = cs1)\n\n            paraDs = data.frame(matrix(NA, nrow = 3, ncol = 1))\n            colnames(paraDs) = \"name\"\n            row.names(paraDs) = c(\"Model.Package\", \"XB.Size\", \"XB.SamplingMethod\")\n            paraDs[\"Model.Package\",\"name\"] = self$Model.Package\n            paraDs[\"XB.Size\",\"name\"] = self$XB.Size\n            paraDs[\"XB.SamplingMethod\",\"name\"] = self$XB.SamplingMethod\n            nr = nrow(self$ParaTable) + 3\n            xlsx::addDataFrame(x = paraDs, sheet = sheet,\n                               row.names = TRUE, col.names = FALSE,\n                               startColumn = 1, startRow = nr,\n                               rownamesStyle = cs2)\n\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n            for(fea in AList){\n                clusRes = self$Clustering.Res[[fea]]\n                treeRules = self$TreeRules[[fea]]\n                # import parameters\n                bestK = clusRes$bestK\n                BVar = as.character(treeRules$depenVar)\n\n                AData = self$XA.Sample[[fea]]\n                clusterData = self$Pred.Res[[fea]]\n                BData = self$XB.Sample\n                groups = clusRes$groups\n                IPPs = clusRes$medoids\n                if(!is.factor(AData)){\n                    xlabel = AData\n                }else{\n                    xlabel = as.character(AData)\n                }\n\n                #-----create worksheet-----\n                sheet  <- xlsx::createSheet(wb, sheetName=fea)\n\n                row.n = 1\n                #-----write impact pattern plots-----\n                xlsx::addDataFrame(x = data.frame(\"IMPACT PATTERNS\"), sheet = sheet,\n                                   row.names = FALSE, col.names = FALSE,\n                                   startColumn = 1, startRow = row.n, colStyle = list('1'=cs1))\n                row.n = row.n + 1\n\n                data = data.frame(t(xlabel))\n                rownames(data) = fea\n                xlsx::addDataFrame(x = data, sheet = sheet,\n                                   row.names = TRUE, col.names = FALSE,\n                                   startColumn = 1, startRow = row.n)\n                row.n = row.n + 1\n\n                row.names(IPPs) = paste(\"cluster\", 1:bestK, sep = \"-\")\n                xlsx::addDataFrame(x = IPPs, sheet = sheet,\n                                   row.names = TRUE, col.names = FALSE,\n                                   startColumn = 1, startRow = row.n)\n                row.n = row.n + nrow(IPPs) + 2\n\n                #-----write cluster information-----\n                xlsx::addDataFrame(x = data.frame(\"CLUSTER INFORMATION\"), sheet = sheet,\n                                   row.names = FALSE, col.names = FALSE,\n                                   startColumn = 1, startRow = row.n, colStyle = list('1'=cs1))\n                row.n = row.n + 1\n                for(j in 1:bestK){\n                    xlsx::addDataFrame(x = data.frame(paste(\"cluster\",j,sep = '-')), sheet = sheet,\n                                       row.names = FALSE, col.names = FALSE,\n                                       startColumn = 1, startRow = row.n,colStyle = list('1'=cs2))\n\n                    row.n = row.n + 1\n                    data = data.frame(t(xlabel))\n                    rownames(data) = fea\n                    xlsx::addDataFrame(x = data, sheet = sheet,\n                                       row.names = TRUE, col.names = FALSE,\n                                       startColumn = 1, startRow = row.n)\n\n                    if(length(which(groups == j)) > 1){\n                        minL = apply(clusterData[groups == j,],2,min)\n                        maxL = apply(clusterData[groups == j,],2,max)\n                        avgL = apply(clusterData[groups == j,],2,mean)\n                    }else{\n                        minL = clusterData[groups == j,]\n                        maxL = minL\n                        avgL = minL\n                    }\n\n                    data = rbind(minL,avgL,maxL)\n                    rownames(data) = c(\"min\",\"mean\",\"max\")\n\n                    row.n = row.n + 1\n                    xlsx::addDataFrame(x = data, sheet = sheet,\n                                       row.names = TRUE, col.names = FALSE,\n                                       startColumn = 1, startRow = row.n)\n                    row.n = row.n + 4\n                }\n\n                #-----write tree rules-----\n                if(bestK > 1){\n                    row.n = row.n + 1\n                    xlsx::addDataFrame(x = data.frame(\"TREE RULES\"), sheet = sheet,\n                                       row.names = FALSE, col.names = FALSE,\n                                       startColumn = 1, startRow = row.n, colStyle = list('1'=cs1))\n                    row.n = row.n + 1\n                    xlsx::addDataFrame(x = treeRules$rules, sheet = sheet,\n                                       row.names = FALSE, col.names = TRUE,\n                                       startColumn = 1, startRow = row.n)\n\n                    if(length(BVar) > 0){ # tree rules exist\n                        row.n = row.n + nrow(treeRules$rules) + 1\n                        xlsx::addDataFrame(x = paste(\"maxDepth = \", treeRules$maxDepth),\n                                           sheet = sheet,\n                                           row.names = FALSE, col.names = FALSE,\n                                           startColumn = 1, startRow = row.n)\n                    }\n\n                }\n                else{ # only one cluster\n                    row.n = row.n + 1\n                    xlsx::addDataFrame(x = data.frame(\"NO RULES\"), sheet = sheet,\n                                       row.names = FALSE, col.names = FALSE,\n                                       startColumn = 1, startRow = row.n, colStyle = list('1'=cs1))\n                }\n            }\n\n            # write feature interaction network (FIN)\n            sheet  <- xlsx::createSheet(wb, sheetName=\"FIN\")\n            xlsx::addDataFrame(x = data.frame(\"FROM\"), sheet = sheet,\n                               row.names = FALSE, col.names = FALSE,\n                               startColumn = 1, startRow = 3, colStyle = list('1'=cs1))\n            xlsx::addDataFrame(x = data.frame(\"TO\"), sheet = sheet,\n                               row.names = FALSE, col.names = FALSE,\n                               startColumn = 3, startRow = 1, colStyle = list('1'=cs1))\n            xlsx::addDataFrame(x = self$FIN.Data, sheet = sheet,\n                               row.names = TRUE, col.names = TRUE,\n                               startColumn = 2, startRow = 2)\n\n            # save data\n            xlsx::saveWorkbook(wb, excelName)\n\n            cat(\"   ...Finished\\n\")\n        },\n\n        #------------------------------------\n        # draw feature interaction network (FIN)\n        #------------------------------------\n        DrawFIN = function(threshold = 0, lay.out = igraph::layout.auto){\n            if(!is.data.frame(self$FIN.Data)){\n                stop(\"FIN data is not correct. You need to build decision tree before drawing FIN.\")\n            }\n            # create grpah matrix\n            netMatrix = as.matrix(self$FIN.Data)\n            netMatrix[is.na(netMatrix)] <- 0\n            netMatrix[netMatrix < threshold] <- 0\n            # draw network\n            g = igraph::graph_from_adjacency_matrix(netMatrix, mode = \"directed\", weighted = TRUE)\n            igraph::plot.igraph(g, layout=lay.out)\n        },\n\n        #------------------------------------\n        # Sampling, Prediting, Clustering and Building Tree\n        #------------------------------------\n        ExecuteAll = function(){\n            self$SamplingXA()\n\n            self$SamplingXB()\n\n            self$PredictData()\n\n            self$ClusterImpactPlots()\n\n            self$BuildTree()\n        }\n    ),\n\n    private = list(\n        #------------------------------------\n        # prediction of X_A and X_B for single feature\n        #------------------------------------\n        DataPredictionSingle = function(X){\n            require(self$Model.Package,character.only = TRUE)\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n            fea = AList[X]\n            res = list()\n            X.ds = self$XB.Sample\n            for(i in 1:length(self$XA.Sample[[fea]])){ # Cardian product of XB.Sample and XA.Sample\n                x.a = self$XA.Sample[[fea]][i]\n                X.ds[fea] = x.a\n\n                predRes = predict(self$Pred.Fun, X.ds, type=self$Pred.Type) # predict\n                if(is.vector(predRes)){ # regression problem\n                    res[[as.character(x.a)]] =  predRes\n                }else{ # classification problem\n                    if(ncol(predRes) < self$Pred.Dimension)\n                        self$Pred.Dimension = 1\n                    res[[as.character(x.a)]] =  predRes[,self$Pred.Dimension]\n                }\n            }\n            return (as.data.frame(res))\n        },\n\n        #------------------------------------\n        # calculate Cosine similarity\n        #------------------------------------\n        CosineSim = function(X){\n            n = ncol(X)\n            sim = matrix(NA,n,n)\n            for(i in 1:(n-1)){\n                sim[i,i] = 1\n                for(j in (i+1):n){\n                    sim[i,j] = sum(X[,i] * X[,j]) /\n                        (sqrt(sum(X[,i] * X[,i])) * sqrt(sum(X[,j] * X[,j])))\n                    sim[j,i] = sim[i,j]\n                }\n            }\n            sim[n,n] = 1\n            return (sim)\n        },\n\n        #------------------------------------\n        # clustering for single feature\n        #------------------------------------\n        ClusteringSingle = function(X){\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n            fea = AList[X]\n\n            clusData = self$Pred.Res[[fea]]\n            original.clusData = clusData\n            centralized = self$ParaTable[fea, \"centralized\"]\n            distMeasure = self$ParaTable[fea, \"distMeasure\"]\n            autoK = self$ParaTable[fea, \"autoK\"]\n            numK = self$ParaTable[fea, \"numK\"]\n            clusterMethod = self$ParaTable[fea, \"clusteringMethod\"]\n\n            # -----Centralize the dataset-----\n            if(centralized){\n                clusData = clusData - rowMeans(clusData)\n            }\n            if(distMeasure == \"cosine\"){\n                for(i in 1:nrow(clusData)){\n                    if(sum(clusData[i,]* clusData[i,]) == 0)\n                        clusData[i,] = 0.001\n                }\n            }\n\n            # -----Calculate the distance matrix-----\n            if(autoK | clusterMethod == 'kmedoids'){\n                if(distMeasure == \"euclidean\")\n                    distMatrix = dist(clusData)\n                if(distMeasure == \"cosine\")\n                    distMatrix = 1 - private$CosineSim(t(clusData))\n\n                if(all(distMatrix < 0.00001)){\n                    if(clusterMethod == 'kmeans')\n                        return (list(groups = rep(1, self$XB.Size), medoids = as.data.frame(colMeans(clusData)), bestK = 1))\n                    else\n                        return (list(groups = rep(1, self$XB.Size), medoids = original.clusData[1,], bestK = 1))\n                }\n            }\n\n            # -----Clustering by k-medoids method-----\n            if(autoK){ # the number of clusters are determined through Dunn index\n                clusRes = list()\n                bestDunn = 0\n                bestK = 1\n                for(k in 2:numK){\n                    # clustering\n                    if(clusterMethod == 'kmeans'){ #k-means\n                        if(distMeasure == \"euclidean\"){\n                            temp = akmeans::akmeans(x = clusData, min.k = k, max.k = k, d.metric = 1, ths1 = 1e10)\n                            clusRes[[k]] = list(clustering = temp$cluster,medoids = temp$centers)\n                        }else{ # cosin\n                            temp = akmeans::akmeans(x = clusData, min.k = k, max.k = k, d.metric = 2, ths1 = 1e10)\n                            clusRes[[k]] = list(clustering = temp$cluster,medoids = temp$centers)\n                        }\n\n                    }else{ # k-medoid\n                        temp = cluster::pam(x = distMatrix,k = k, diss = TRUE)\n                        clusRes[[k]] = list(clustering = temp$clustering,medoids = temp$medoids)\n                    }\n\n                    # calculate dunn to determine the number of clusters\n                    clusterIndex = fpc::cluster.stats(d = distMatrix, clustering = clusRes[[k]]$clustering,\n                                                      silhouette = FALSE)$dunn\n\n                    if(is.infinite(clusterIndex)){\n                        if(clusterMethod == 'kmedoids')\n                            return(list(groups = rep(1,self$XB.Size), medoids = original.clusData[1,], bestK = 1))\n                        else\n                            return(list(groups = rep(1,self$XB.Size), medoids = as.data.frame(colMeans(clusData)), bestK = 1))\n                    }\n\n                    if(clusterIndex > bestDunn){\n                        bestDunn = clusterIndex\n                        bestK = k\n                    }\n                    groups = clusRes[[bestK]]$clustering\n                    if(clusterMethod == 'kmeans')\n                        medoids = clusRes[[bestK]]$medoids\n                    else\n                        medoids = original.clusData[clusRes[[bestK]]$medoids,]\n                }\n            }else{ # the numbe of clusters are specified\n                bestK = numK\n                if(clusterMethod == 'kmeans'){\n                    if(distMeasure == \"euclidean\"){\n                        temp = akmeans::akmeans(x = clusData, min.k = bestK, max.k = bestK, d.metric = 1, ths1 = 1e10)\n                    }else{ # cosin\n                        temp = akmeans::akmeans(x = clusData, min.k = bestK, max.k = bestK, d.metric = 2, ths1 = 1e10)\n                    }\n                    groups = temp$cluster\n                    medoids = temp$centers\n                }else{\n                    temp = cluster::pam(x = distMatrix,k = bestK, diss = TRUE)\n                    groups = temp$clustering\n                    medoids = original.clusData[temp$medoids,]\n                }\n            }\n            if(bestK == 1){ # only one cluster\n                groups = rep(1,self$XB.Size)\n                if(clusterMethod == 'kmeans')\n                    medoids = colMeans(clusData)\n                else\n                    medoids = original.clusData[1,]\n            }\n            return (list(groups = groups, medoids = as.data.frame(medoids), bestK = bestK))\n\n        },\n\n        #------------------------------------\n        # build decision tree for single feature\n        #------------------------------------\n        BuildingTreeSingle = function(X){\n            AList = rownames(self$ParaTable)[self$ParaTable[\"X_A\"] == TRUE]\n            feaList = names(self$X.Data)\n\n            fea = AList[X]\n            maxDepth = self$ParaTable[fea, \"treeDepth\"]\n            minSplit = self$ParaTable[fea, \"minSplit\"]\n            groups = self$Clustering.Res[[fea]][[\"groups\"]]\n            bestK = self$Clustering.Res[[fea]][[\"bestK\"]]\n            BData = self$XB.Sample[feaList[! fea == feaList]]\n\n            # there is only one cluster\n            if(bestK == 1){\n                return (list(rules = NA,depenVar = NA,maxDepth = NA,varImp = NA))\n            }\n\n            #-----build tree model-----\n            md = maxDepth\n            continue = TRUE\n            while(continue){\n                treeModel = rpart::rpart(groups ~ .,data = BData, method = \"class\",\n                                         control = list(maxdepth = md, minsplit = minSplit))\n                temp = treeModel$frame\n                leafNum = rownames(temp)[temp[\"var\"] == \"<leaf>\"]\n\n                if(length(leafNum) == 1 & md < 2 * maxDepth){\n                    md = md + 1\n                }else{\n                    continue = FALSE\n                }\n            }\n\n            # if(length(leafNum) == 1){\n            #     return (list(rules = NA,depenVar = NA,maxDepth = NA,varImp = NA))\n            # }\n\n            #-----add links to FIN-----\n            varImp = treeModel$variable.importance\n            varImp = varImp / sum(varImp)\n\n            #-----extract rules-----\n            temp = treeModel$frame\n            leafNum = rownames(temp)[temp[\"var\"] == \"<leaf>\"]\n            depenVar = unique(temp[temp[\"var\"] != \"<leaf>\",\"var\"])\n            rules = rpart::path.rpart(treeModel,leafNum,print.it = FALSE)\n            rules.d = rep(NA,length(rules))\n            for(i in 1:length(rules))\n                rules.d[i] = paste(rules[[i]][-1],collapse = \" & \")\n\n            #-----extract the distribution of class in each leaf node (rule)-----\n            unNum = sort(unique(treeModel$where))\n            seqNum = data.frame('num' = 1:length(unNum))\n            row.names(seqNum) = unNum\n            xyz = data.frame(\"class\" = seqNum[as.character(treeModel$where),\"num\"],groups)\n            xx = as.data.frame.matrix(table(xyz))\n            colnames(xx) = paste(\"clu\", names(xx),sep = '-')\n\n            return (list(rules = data.frame(\"rules\" = rules.d,xx),\n                         depenVar = depenVar,maxDepth = md,\n                         varImp = varImp))\n        }\n    )\n)\n\n\n",
    "created" : 1521472608298.000,
    "dirty" : false,
    "encoding" : "CP936",
    "folds" : "171|57|198|8|\n203|105|243|8|\n248|39|311|8|\n316|36|393|8|\n398|32|438|8|\n443|32|472|8|\n477|33|507|8|\n512|40|540|8|\n545|31|583|8|\n588|51|638|8|\n643|43|793|8|\n798|73|809|8|\n814|32|824|8|\n",
    "hash" : "3815422166",
    "id" : "A8753113",
    "lastKnownWriteTime" : 1521473000,
    "path" : "~/zxhang/IPPModel/R/IPPModel.R",
    "project_path" : "R/IPPModel.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}